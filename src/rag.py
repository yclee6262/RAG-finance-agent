import os
import torch
import pickle # <--- æ–°å¢
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import LocalFileStore, EncoderBackedStore # <--- æ–°å¢
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# è¨­å®šè·¯å¾‘
INDEX_PATH = "data/faiss_index"
DOCSTORE_PATH = "data/doc_store"

def get_embedding_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

def load_system():
    """è¼‰å…¥ RAG ç³»çµ± (Retriever + LLM)"""
    print("æ­£åœ¨è¼‰å…¥ RAG ç³»çµ±...")
    
    # 1. è¼‰å…¥ Embedding
    embedding_model = get_embedding_model()
    
    # 2. è¼‰å…¥ FAISS å‘é‡åº«
    if not os.path.exists(INDEX_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº« {INDEX_PATH}ï¼Œè«‹å…ˆåŸ·è¡Œ retriever.py")
        
    vectorstore = FAISS.load_local(
        INDEX_PATH, 
        embedding_model,
        allow_dangerous_deserialization=True
    )
    
    # 3. è¼‰å…¥çˆ¶æ–‡æª”åº« (LocalFileStore + Encoder) <--- é—œéµä¿®æ”¹
    fs = LocalFileStore(DOCSTORE_PATH)
    docstore = EncoderBackedStore(
        store=fs,
        key_encoder=lambda x: x,
        value_serializer=pickle.dumps,
        value_deserializer=pickle.loads
    )
    
    # 4. é‡å»º Retriever
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    
    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=docstore,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
        search_kwargs={"k": 6} 
    )
    
    # 5. è¨­å®š LLM (Ollama - LLaMA 3)
    llm = ChatOllama(
        model="llama3", 
        temperature=0.1, 
    )
    
    # 6. è¨­å®š Prompt Template (å¼·åŒ–ç‰ˆï¼šå¼·åˆ¶ä¸­æ–‡ + æ€ç¶­éˆ)
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é‡‘èåˆ†æå¸«ï¼Œè«‹å”åŠ©å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    ã€åš´æ ¼éµå®ˆè¦å‰‡ã€‘
    1. **èªè¨€é™åˆ¶**ï¼šé™¤éå°ˆæœ‰åè©ï¼Œå¦å‰‡**æ‰€æœ‰å›ç­”å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ã€‚ç¦æ­¢ä½¿ç”¨è‹±æ–‡ä½œç­”ã€‚
    2. **è¡¨æ ¼é–±è®€ç­–ç•¥**ï¼š
       - è‹¥å•é¡Œæ¶‰åŠã€Œç¯©é¸æ¢ä»¶ã€ï¼ˆå¦‚ï¼šæŒè‚¡ > 20%ï¼‰ï¼Œè«‹å‹™å¿…**æƒæè¡¨æ ¼çš„æ¯ä¸€åˆ—**ï¼Œä¸è¦åªçœ‹å‰å¹¾è¡Œã€‚
       - è«‹æ‰¾å‡º**æ‰€æœ‰**ç¬¦åˆæ¢ä»¶çš„é …ç›®ï¼Œä¸è¦éºæ¼ã€‚
       - è‹¥è¡¨æ ¼ä¸­çš„æ•¸å­—æœ‰æ‹¬è™Ÿï¼ˆå¦‚ (0.36)ï¼‰ï¼Œä»£è¡¨è² æ•¸æˆ–æ¸›å°‘ã€‚
    3. **æ•¸æ“šç²¾ç¢ºæ€§**ï¼šå›ç­”ä¸­çš„æ•¸å­—å¿…é ˆèˆ‡æ–‡ä»¶å…§å®¹å®Œå…¨ä¸€è‡´ã€‚
    4. **ç„¡ç­”æ¡ˆè™•ç†**ï¼šè‹¥æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°è³‡è¨Šï¼Œè«‹ç›´æ¥å›ç­”ã€Œæ ¹æ“šç¾æœ‰æ–‡ä»¶ç„¡æ³•å›ç­”ã€ã€‚

    ã€åƒè€ƒæ–‡ä»¶ç‰‡æ®µã€‘
    {context}

    ã€ä½¿ç”¨è€…å•é¡Œã€‘
    {question}

    ã€ä½ çš„åˆ†æèˆ‡å›ç­”ã€‘(è«‹ç”¨ç¹é«”ä¸­æ–‡)ï¼š"""
    
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    # 7. å»ºç«‹ QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    
    return qa_chain

if __name__ == "__main__":
    qa_chain = load_system()
    
    print("\nâœ… RAG ç³»çµ±å·²å°±ç·’ï¼è«‹è¼¸å…¥é—œæ–¼è‡ºéŠ€å¹´å ±çš„å•é¡Œ (è¼¸å…¥ 'exit' é›¢é–‹)")
    print("-" * 50)
    
    while True:
        query = input("\nè«‹è¼¸å…¥å•é¡Œ: ")
        if query.lower() in ['exit', 'quit']:
            break
            
        print("æ­£åœ¨æ€è€ƒä¸­...")
        result = qa_chain.invoke({"query": query})
        
        print("\nğŸ¤– å›ç­”:")
        print(result['result'])
        
        print("\nğŸ“„ åƒè€ƒä¾†æºç‰‡æ®µ:")
        for doc in result['source_documents']:
            print(f"- ...{doc.page_content[:50]}...")